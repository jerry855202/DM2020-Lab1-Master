{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Lab 1\n",
    "In this lab session we will focus on the use of scientific computing libraries to efficiently process, transform, and manage data. Furthermore, we will provide best practices and introduce visualization tools for effectively conducting big data analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Data Source\n",
    "2. Data Preparation\n",
    "3. Data Transformation\n",
    " - 3.1 Converting Dictionary into Pandas dataframe\n",
    " - 3.2 Familiarizing yourself with the Data\n",
    "4. Data Mining using Pandas\n",
    " - 4.1 Dealing with Missing Values\n",
    " - 4.2 Dealing with Duplicate Data\n",
    "5. Data Preprocessing\n",
    " - 5.1 Sampling\n",
    " - 5.2 Feature Creation\n",
    " - 5.3 Feature Subset Selection\n",
    " - 5.4 Dimensionality Reduction\n",
    " - 5.5 Atrribute Transformation / Aggregation\n",
    " - 5.6 Discretization and Binarization\n",
    "6. Data Exploration\n",
    "7. Conclusion\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook I will explore a text-based, document-based [dataset](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) using scientific computing tools such as Pandas and Numpy. In addition, several fundamental Data Mining concepts will be explored and explained in details, ranging from calculating distance measures to computing term frequency vectors. Coding examples, visualizations and demonstrations will be provided where necessary. Furthermore, additional exercises are provided after special topics. These exercises are geared towards testing the proficiency of students and motivate students to explore beyond the techniques covered in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Here are the computing and software requirements\n",
    "\n",
    "#### Computing Resources\n",
    "- Operating system: Preferably Linux or MacOS\n",
    "- RAM: 8 GB\n",
    "- Disk space: Mininium 8 GB\n",
    "\n",
    "#### Software Requirements\n",
    "Here is a list of the required programs and libraries necessary for this lab session:\n",
    "\n",
    "##### Language:\n",
    "- [Python 3+](https://www.python.org/download/releases/3.0/) (Note: coding will be done strictly on Python 3)\n",
    "    - Install latest version of Python 3\n",
    "    \n",
    "##### Environment:\n",
    "Using an environment is to avoid some library conflict problems. You can refer this [Setup Instructions](http://cs231n.github.io/setup-instructions/) to install and setup.\n",
    "\n",
    "- [Anaconda](https://www.anaconda.com/download/) (recommended but not required)\n",
    "    - Install anaconda environment\n",
    "    \n",
    "- [Python virtualenv](https://virtualenv.pypa.io/en/stable/userguide/) (recommended to Linux/MacOS user)\n",
    "    - Install virtual environment\n",
    "\n",
    "- [Kaggle Kernel](https://www.kaggle.com/kernels/)\n",
    "    - Run on the cloud  (with some limitations)\n",
    "    - Reference: [Kaggle Kernels Instructions](https://github.com/omarsar/data_mining_lab/blob/master/kagglekernel.md)\n",
    "    \n",
    "##### Necessary Libraries:\n",
    "- [Jupyter](http://jupyter.org/) (Strongly recommended but not required)\n",
    "    - Install `jupyter` and Use `$jupyter notebook` in terminal to run\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/index.html)\n",
    "    - Install `sklearn` latest python library\n",
    "- [Pandas](http://pandas.pydata.org/)\n",
    "    - Install `pandas` python library\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "    - Install `numpy` python library\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "    - Install `maplotlib` for python\n",
    "- [Plotly](https://plot.ly/)\n",
    "    - Install and signup for `plotly`\n",
    "- [Seaborn](https://seaborn.pydata.org/)\n",
    "    - Install and signup for `seaborn`\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - Install `nltk` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data\n",
    "In this notebook we will explore the popular 20 newsgroup dataset, originally provided [here](http://qwone.com/~jason/20Newsgroups/). The dataset is called \"Twenty Newsgroups\", which means there are 20 categories of news articles available in the entire dataset. A short description of the dataset, provided by the authors, is provided below:\n",
    "\n",
    "- *The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.*\n",
    "\n",
    "If you need more information about the dataset please refer to the reference provided above. Below is a snapshot of the dataset already converted into a table. Keep in mind that the original dataset is not in this nice pretty format. That work is left to us. That is one of the tasks that will be covered in this notebook: how to convert raw data into convenient tabular formats using Pandas. \n",
    "\n",
    "![atl txt](https://docs.google.com/drawings/d/e/2PACX-1vRd845nNXa1x1Enw6IoEbg-05lB19xG3mfO2BjnpZrloT0pSnY89stBV1gS9Iu6cgRCTq3E5giIT5ZI/pub?w=835&h=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "Now let us begin to explore the data. The original dataset can be found on the link provided above or you can directly use the version provided by scikit learn. Here we will use the scikit learn version. \n",
    "\n",
    "In this demonstration we are only going to look at 4 categories. This means we will not make use of the complete dataset, but only a subset of it, which includes the 4 categories defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the documents containing the categories provided\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \\\n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take at look some of the records that are contained in our subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n&#39;,\n &quot;From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the &#39;gurus&#39; can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n&quot;]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "twenty_train.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the `twenty_train` is just a bunch of objects that can be accessed as python dictionaries; so, you can do the following operations on `twenty_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;sci.med&#39;, &#39;soc.religion.christian&#39;]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2257"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2257"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also print an example from the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "From: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\nOrganization: The City University\nLines: 14\n\nDoes anyone know of a good way (standard PC application/PD utility) to\nconvert tif/img/tga files into LaserJet III format.  We would also like to\ndo the same, converting to HPGL (HP plotter) files.\n\nPlease email any response.\n\nIs this the correct group?\n\nThanks in advance.  Michael.\n-- \nMichael Collier (Programmer)                 The Computer Unit,\nEmail: M.P.Collier@uk.ac.city                The City University,\nTel: 071 477-8000 x3769                      London,\nFax: 071 477-8565                            EC1V 0HB.\n\n"
    }
   ],
   "source": [
    "# An example of what the subset contains\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and determine the label of the example via `target_names` key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "comp.graphics\n"
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "twenty_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can also get the category of 10 documents via `target` key value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# category of first 10 documents.\n",
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As you can observe, both approaches above provide two different ways of obtaining the `category` value for the dataset. Ideally, we want to have access to both types -- numerical and nominal -- in the event some particular library favors a particular type. \n",
    "\n",
    "As you may have already noticed as well, there is no **tabular format** for the current version of the data. As data miners, we are interested in having our dataset in the most convenient format as possible; something we can manipulate easily and is compatible with our algorithms, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one way to get access to the *text* version of the label of a subset of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "comp.graphics\ncomp.graphics\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsoc.religion.christian\nsci.med\nsci.med\nsci.med\n"
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 1 (5 min): **  \n",
    "In this exercise, please print out the *text* data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "From: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\nOrganization: The City University\nLines: 14\n\nDoes anyone know of a good way (standard PC application/PD utility) to\nconvert tif/img/tga files into LaserJet III format.  We would also like to\ndo the same, converting to HPGL (HP plotter) files.\n\nPlease email any response.\n\nIs this the correct group?\n\nThanks in advance.  Michael.\n-- \nMichael Collier (Programmer)                 The Computer Unit,\nEmail: M.P.Collier@uk.ac.city                The City University,\nTel: 071 477-8000 x3769                      London,\nFax: 071 477-8565                            EC1V 0HB.\n\nFrom: ani@ms.uky.edu (Aniruddha B. Deglurkar)\nSubject: help: Splitting a trimming region along a mesh \nOrganization: University Of Kentucky, Dept. of Math Sciences\nLines: 28\n\n\n\n\tHi,\n\n\tI have a problem, I hope some of the &#39;gurus&#39; can help me solve.\n\n\tBackground of the problem:\n\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n\tmapping of a 3d Bezier patch into 2d. The area in this domain\n\twhich is inside a trimming loop had to be rendered. The trimming\n\tloop is a set of 2d Bezier curve segments.\n\tFor the sake of notation: the mesh is made up of cells.\n\n\tMy problem is this :\n\tThe trimming area has to be split up into individual smaller\n\tcells bounded by the trimming curve segments. If a cell\n\tis wholly inside the area...then it is output as a whole ,\n\telse it is trivially rejected. \n\n\tDoes any body know how thiss can be done, or is there any algo. \n\tsomewhere for doing this.\n\n\tAny help would be appreciated.\n\n\tThanks, \n\tAni.\n-- \nTo get irritated is human, to stay cool, divine.\n\nFrom: djohnson@cs.ucsd.edu (Darin Johnson)\nSubject: Re: harrassed at work, could use some prayers\nOrganization: =CSE Dept., U.C. San Diego\nLines: 63\n\n(Well, I&#39;ll email also, but this may apply to other people, so\nI&#39;ll post also.)\n\n&gt;I&#39;ve been working at this company for eight years in various\n&gt;engineering jobs.  I&#39;m female.  Yesterday I counted and realized that\n&gt;on seven different occasions I&#39;ve been sexually harrassed at this\n&gt;company.\n\n&gt;I dreaded coming back to work today.  What if my boss comes in to ask\n&gt;me some kind of question...\n\nYour boss should be the person bring these problems to.  If he/she\ndoes not seem to take any action, keep going up higher and higher.\nSexual harrassment does not need to be tolerated, and it can be an\nenormous emotional support to discuss this with someone and know that\nthey are trying to do something about it.  If you feel you can not\ndiscuss this with your boss, perhaps your company has a personnel\ndepartment that can work for you while preserving your privacy.  Most\ncompanies will want to deal with this problem because constant anxiety\ndoes seriously affect how effectively employees do their jobs.\n\nIt is unclear from your letter if you have done this or not.  It is\nnot inconceivable that management remains ignorant of employee\nproblems/strife even after eight years (it&#39;s a miracle if they do\nnotice).  Perhaps your manager did not bring to the attention of\nhigher ups?  If the company indeed does seem to want to ignore the\nentire problem, there may be a state agency willing to fight with\nyou.  (check with a lawyer, a women&#39;s resource center, etc to find out)\n\nYou may also want to discuss this with your paster, priest, husband,\netc.  That is, someone you know will not be judgemental and that is\nsupportive, comforting, etc.  This will bring a lot of healing.\n\n&gt;So I returned at 11:25, only to find that ever single\n&gt;person had already left for lunch.  They left at 11:15 or so.  No one\n&gt;could be bothered to call me at the other building, even though my\n&gt;number was posted.\n\nThis happens to a lot of people.  Honest.  I believe it may seem\nto be due to gross insensitivity because of the feelings you are\ngoing through.  People in offices tend to be more insensitive while\nworking than they normally are (maybe it&#39;s the hustle or stress or...)\nI&#39;ve had this happen to me a lot, often because they didn&#39;t realize\nmy car was broken, etc.  Then they will come back and wonder why I\ndidn&#39;t want to go (this would tend to make me stop being angry at\nbeing ignored and make me laugh).  Once, we went off without our\nboss, who was paying for the lunch :-)\n\n&gt;For this\n&gt;reason I hope good Mr. Moderator allows me this latest indulgence.\n\nWell, if you can&#39;t turn to the computer for support, what would\nwe do?  (signs of the computer age :-)\n\nIn closing, please don&#39;t let the hateful actions of a single person\nharm you.  They are doing it because they are still the playground\nbully and enjoy seeing the hurt they cause.  And you should not\naccept the opinions of an imbecile that you are worthless - much\nwiser people hold you in great esteem.\n-- \nDarin Johnson\ndjohnson@ucsd.edu\n  - Luxury!  In MY day, we had to make do with 5 bytes of swap...\n\n"
    }
   ],
   "source": [
    "# Answer here\n",
    "for i in range(3):\n",
    "    print(twenty_train.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n",
    "So we want to explore and understand our data a little bit better. Before we do that we definitely need to apply some transformations just so we can have our dataset in a nice format to be able to explore it freely and more efficient. Lucky for us, there are powerful scientific tools to transform our data into that tabular format we are so farmiliar with. So that is what we will do in the next section--transform our data into a nice table format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting Dictionary into Pandas Dataframe\n",
    "Here we will show you how to convert dictionary objects into a pandas dataframe. And by the way, a pandas dataframe is nothing more than a table magically stored for efficient information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n&#39;,\n &quot;From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the &#39;gurus&#39; can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n&quot;]"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "twenty_train.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 3, ..., 2, 2, 2], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# construct dataframe from a list\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2257"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text\n0  From: sd345@city.ac.uk (Michael Collier) Subje...\n1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "From: sd345@city.ac.uk (Michael Collier) Subject: Converting images to HP LaserJet III? Nntp-Posting-Host: hampton Organization: The City University Lines: 14  Does anyone know of a good way (standard PC application/PD utility) to convert tif/img/tga files into LaserJet III format.  We would also like to do the same, converting to HPGL (HP plotter) files.  Please email any response.  Is this the correct group?  Thanks in advance.  Michael. --  Michael Collier (Programmer)                 The Computer Unit, Email: M.P.Collier@uk.ac.city                The City University, Tel: 071 477-8000 x3769                      London, Fax: 071 477-8565                            EC1V 0HB. \nFrom: ani@ms.uky.edu (Aniruddha B. Deglurkar) Subject: help: Splitting a trimming region along a mesh  Organization: University Of Kentucky, Dept. of Math Sciences Lines: 28    \tHi,  \tI have a problem, I hope some of the &#39;gurus&#39; can help me solve.  \tBackground of the problem: \tI have a rectangular mesh in the uv domain, i.e  the mesh is a  \tmapping of a 3d Bezier patch into 2d. The area in this domain \twhich is inside a trimming loop had to be rendered. The trimming \tloop is a set of 2d Bezier curve segments. \tFor the sake of notation: the mesh is made up of cells.  \tMy problem is this : \tThe trimming area has to be split up into individual smaller \tcells bounded by the trimming curve segments. If a cell \tis wholly inside the area...then it is output as a whole , \telse it is trivially rejected.   \tDoes any body know how thiss can be done, or is there any algo.  \tsomewhere for doing this.  \tAny help would be appreciated.  \tThanks,  \tAni. --  To get irritated is human, to stay cool, divine. \nFrom: djohnson@cs.ucsd.edu (Darin Johnson) Subject: Re: harrassed at work, could use some prayers Organization: =CSE Dept., U.C. San Diego Lines: 63  (Well, I&#39;ll email also, but this may apply to other people, so I&#39;ll post also.)  &gt;I&#39;ve been working at this company for eight years in various &gt;engineering jobs.  I&#39;m female.  Yesterday I counted and realized that &gt;on seven different occasions I&#39;ve been sexually harrassed at this &gt;company.  &gt;I dreaded coming back to work today.  What if my boss comes in to ask &gt;me some kind of question...  Your boss should be the person bring these problems to.  If he/she does not seem to take any action, keep going up higher and higher. Sexual harrassment does not need to be tolerated, and it can be an enormous emotional support to discuss this with someone and know that they are trying to do something about it.  If you feel you can not discuss this with your boss, perhaps your company has a personnel department that can work for you while preserving your privacy.  Most companies will want to deal with this problem because constant anxiety does seriously affect how effectively employees do their jobs.  It is unclear from your letter if you have done this or not.  It is not inconceivable that management remains ignorant of employee problems/strife even after eight years (it&#39;s a miracle if they do notice).  Perhaps your manager did not bring to the attention of higher ups?  If the company indeed does seem to want to ignore the entire problem, there may be a state agency willing to fight with you.  (check with a lawyer, a women&#39;s resource center, etc to find out)  You may also want to discuss this with your paster, priest, husband, etc.  That is, someone you know will not be judgemental and that is supportive, comforting, etc.  This will bring a lot of healing.  &gt;So I returned at 11:25, only to find that ever single &gt;person had already left for lunch.  They left at 11:15 or so.  No one &gt;could be bothered to call me at the other building, even though my &gt;number was posted.  This happens to a lot of people.  Honest.  I believe it may seem to be due to gross insensitivity because of the feelings you are going through.  People in offices tend to be more insensitive while working than they normally are (maybe it&#39;s the hustle or stress or...) I&#39;ve had this happen to me a lot, often because they didn&#39;t realize my car was broken, etc.  Then they will come back and wonder why I didn&#39;t want to go (this would tend to make me stop being angry at being ignored and make me laugh).  Once, we went off without our boss, who was paying for the lunch :-)  &gt;For this &gt;reason I hope good Mr. Moderator allows me this latest indulgence.  Well, if you can&#39;t turn to the computer for support, what would we do?  (signs of the computer age :-)  In closing, please don&#39;t let the hateful actions of a single person harm you.  They are doing it because they are still the playground bully and enjoy seeing the hurt they cause.  And you should not accept the opinions of an imbecile that you are worthless - much wiser people hold you in great esteem. --  Darin Johnson djohnson@ucsd.edu   - Luxury!  In MY day, we had to make do with 5 bytes of swap... \n"
    }
   ],
   "source": [
    "for t in X[\"text\"][:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great advantages of a pandas dataframe is its flexibility. We can add columns to the current dataset programmatically with very little effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category to the dataframe\n",
    "X['category'] = twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category label also\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print and see what our table looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text  category  \\\n0  From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...         1   \n2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...         3   \n3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...         3   \n4  From: stanly@grok11.columbiasc.ncr.com (stanly...         3   \n5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...         3   \n6  From: jodfishe@silver.ucs.indiana.edu (joseph ...         3   \n7  From: aldridge@netcom.com (Jacquelin Aldridge)...         2   \n8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...         2   \n9  From: libman@hsc.usc.edu (Marlena Libman) Subj...         2   \n\n            category_name  \n0           comp.graphics  \n1           comp.graphics  \n2  soc.religion.christian  \n3  soc.religion.christian  \n4  soc.religion.christian  \n5  soc.religion.christian  \n6  soc.religion.christian  \n7                 sci.med  \n8                 sci.med  \n9                 sci.med  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>From: libman@hsc.usc.edu (Marlena Libman) Subj...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Isn't it? With this format we can conduct many operations easily and efficiently since Pandas dataframes provide us with a wide range of built-in features/functionalities. These features are operations which can directly and quickly be applied to the dataset. These operations may include standard operations like **removing records with missing values** and **aggregating new fields** to the current table (hereinafter referred to as a dataframe), which is desirable in almost every data mining project. Go Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Familiarizing yourself with the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin to show you the awesomeness of Pandas dataframes, let us look at how to run a simple query on our dataset. We want to query for the first 10 rows (documents), and we only want to keep the `text` and `category_name` attributes or fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text           category_name\n0  From: sd345@city.ac.uk (Michael Collier) Subje...           comp.graphics\n1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...           comp.graphics\n2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...  soc.religion.christian\n3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...  soc.religion.christian\n4  From: stanly@grok11.columbiasc.ncr.com (stanly...  soc.religion.christian\n5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...  soc.religion.christian\n6  From: jodfishe@silver.ucs.indiana.edu (joseph ...  soc.religion.christian\n7  From: aldridge@netcom.com (Jacquelin Aldridge)...                 sci.med\n8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...                 sci.med\n9  From: libman@hsc.usc.edu (Marlena Libman) Subj...                 sci.med",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>From: libman@hsc.usc.edu (Marlena Libman) Subj...</td>\n      <td>sci.med</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# a simple query\n",
    "X[0:10][[\"text\", \"category_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few more interesting queries to familiarize ourselves with the efficiency and conveniency of Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's query the last 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   text  category  \\\n2247  From: daniels@math.ufl.edu (TV&#39;s Big Dealer) S...         3   \n2248  From: &quot;danny hawrysio&quot; &lt;danny.hawrysio@canrem....         1   \n2249  From: shellgate!llo@uu4.psi.com (Larry L. Over...         3   \n2250  From: ingles@engin.umich.edu (Ray Ingles) Subj...         0   \n2251  From: Mark-Tarbell@suite.com Subject: Amniocen...         2   \n2252  From: roos@Operoni.Helsinki.FI (Christophe Roo...         2   \n2253  From: mhollowa@ic.sunysb.edu (Michael Holloway...         2   \n2254  From: sasghm@theseus.unx.sas.com (Gary Merrill...         2   \n2255  From: Dan Wallach &lt;dwallach@cs.berkeley.edu&gt; S...         2   \n2256  From: dyer@spdcc.com (Steve Dyer) Subject: Re:...         2   \n\n               category_name  \n2247  soc.religion.christian  \n2248           comp.graphics  \n2249  soc.religion.christian  \n2250             alt.atheism  \n2251                 sci.med  \n2252                 sci.med  \n2253                 sci.med  \n2254                 sci.med  \n2255                 sci.med  \n2256                 sci.med  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2247</td>\n      <td>From: daniels@math.ufl.edu (TV's Big Dealer) S...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>2248</td>\n      <td>From: \"danny hawrysio\" &lt;danny.hawrysio@canrem....</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>2249</td>\n      <td>From: shellgate!llo@uu4.psi.com (Larry L. Over...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>From: ingles@engin.umich.edu (Ray Ingles) Subj...</td>\n      <td>0</td>\n      <td>alt.atheism</td>\n    </tr>\n    <tr>\n      <td>2251</td>\n      <td>From: Mark-Tarbell@suite.com Subject: Amniocen...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2252</td>\n      <td>From: roos@Operoni.Helsinki.FI (Christophe Roo...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2253</td>\n      <td>From: mhollowa@ic.sunysb.edu (Michael Holloway...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2254</td>\n      <td>From: sasghm@theseus.unx.sas.com (Gary Merrill...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2255</td>\n      <td>From: Dan Wallach &lt;dwallach@cs.berkeley.edu&gt; S...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2256</td>\n      <td>From: dyer@spdcc.com (Steve Dyer) Subject: Re:...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "X[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for some sourcery? Brace yourselves! Let us see if we can query every 10th record in our dataframe. In addition, our query must only contain the first 10 records. For this we will use the build-in function called `iloc`. This allows us to query a selection of our dataset by position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 text  category\n0   From: sd345@city.ac.uk (Michael Collier) Subje...         1\n10  From: anasaz!karl@anasazi.com (Karl Dussik) Su...         3\n20  From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...         3\n30  From: vgwlu@dunsell.calgary.chevron.com (greg ...         2\n40  From: david-s@hsr.no (David A. Sjoen) Subject:...         3\n50  From: ab@nova.cc.purdue.edu (Allen B) Subject:...         1\n60  From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...         0\n70  From: weaver@chdasic.sps.mot.com (Dave Weaver)...         3\n80  From: annick@cortex.physiol.su.oz.au (Annick A...         2\n90  Subject: Vonnegut/atheism From: dmn@kepler.unh...         0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>From: anasaz!karl@anasazi.com (Karl Dussik) Su...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>From: vgwlu@dunsell.calgary.chevron.com (greg ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>From: david-s@hsr.no (David A. Sjoen) Subject:...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>From: ab@nova.cc.purdue.edu (Allen B) Subject:...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>From: weaver@chdasic.sps.mot.com (Dave Weaver)...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>From: annick@cortex.physiol.su.oz.au (Annick A...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>Subject: Vonnegut/atheism From: dmn@kepler.unh...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# using loc (by position)\n",
    "X.iloc[::10, 0:2][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `loc` function to explicity define the columns you want to query. Take a look at this [great discussion](https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-ix-vs-at-vs-iat/43968774) on the differences between the `iloc` and `loc` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0     From: sd345@city.ac.uk (Michael Collier) Subje...\n10    From: anasaz!karl@anasazi.com (Karl Dussik) Su...\n20    From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...\n30    From: vgwlu@dunsell.calgary.chevron.com (greg ...\n40    From: david-s@hsr.no (David A. Sjoen) Subject:...\n50    From: ab@nova.cc.purdue.edu (Allen B) Subject:...\n60    From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...\n70    From: weaver@chdasic.sps.mot.com (Dave Weaver)...\n80    From: annick@cortex.physiol.su.oz.au (Annick A...\n90    Subject: Vonnegut/atheism From: dmn@kepler.unh...\nName: text, dtype: object"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# using loc (by label)\n",
    "X.loc[::10, 'text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 text  category  \\\n0   From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n10  From: anasaz!karl@anasazi.com (Karl Dussik) Su...         3   \n20  From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...         3   \n30  From: vgwlu@dunsell.calgary.chevron.com (greg ...         2   \n40  From: david-s@hsr.no (David A. Sjoen) Subject:...         3   \n50  From: ab@nova.cc.purdue.edu (Allen B) Subject:...         1   \n60  From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...         0   \n70  From: weaver@chdasic.sps.mot.com (Dave Weaver)...         3   \n80  From: annick@cortex.physiol.su.oz.au (Annick A...         2   \n90  Subject: Vonnegut/atheism From: dmn@kepler.unh...         0   \n\n             category_name  \n0            comp.graphics  \n10  soc.religion.christian  \n20  soc.religion.christian  \n30                 sci.med  \n40  soc.religion.christian  \n50           comp.graphics  \n60             alt.atheism  \n70  soc.religion.christian  \n80                 sci.med  \n90             alt.atheism  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>From: anasaz!karl@anasazi.com (Karl Dussik) Su...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>From: vgwlu@dunsell.calgary.chevron.com (greg ...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>From: david-s@hsr.no (David A. Sjoen) Subject:...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>From: ab@nova.cc.purdue.edu (Allen B) Subject:...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...</td>\n      <td>0</td>\n      <td>alt.atheism</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>From: weaver@chdasic.sps.mot.com (Dave Weaver)...</td>\n      <td>3</td>\n      <td>soc.religion.christian</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>From: annick@cortex.physiol.su.oz.au (Annick A...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>Subject: Vonnegut/atheism From: dmn@kepler.unh...</td>\n      <td>0</td>\n      <td>alt.atheism</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# standard query (Cannot simultaneously select rows and columns)\n",
    "X[::10][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 3 (5 min): **  \n",
    "Try to fecth records belonging to the ```comp.graphics``` category, and query every 10th record. Only show the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Mining using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some serious work now. Let's learn to program some of the ideas and concepts learned so far in the data mining course. This is the only way we can be convince ourselves of the true power of Pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us consider that our dataset has some *missing values* and we want to remove those values. In its current state our dataset has no missing values, but for practice sake we will add some records with missing values and then write some code to deal with these objects that contain missing values. You will see for yourself how easy it is to deal with missing values once you have your data transformed into a Pandas dataframe.\n",
    "\n",
    "Before we jump into coding, let us do a quick review of what we have learned in the Data Mining course. Specifically, let's review the methods used to deal with missing values.\n",
    "\n",
    "The most common reasons for having missing values in datasets has to do with how the data was initially collected. A good example of this is when a patient comes into the ER room, the data is collected as quickly as possible and depending on the conditions of the patients, the personal data being collected is either incomplete or partially complete. In the former and latter cases, we are presented with a case of \"missing values\". Knowing that patients data is particularly critical and can be used by the health authorities to conduct some interesting analysis, we as the data miners are left with the tough task of deciding what to do with these missing and incomplete records. We need to deal with these records because they are definitely going to affect our analysis or learning algorithms. So what do we do? There are several ways to handle missing values, and some of the more effective ways are presented below (Note: You can reference the slides - Session 1 Handout for the additional information).\n",
    "\n",
    "- **Eliminate Data Objects** - Here we completely discard records once they contain some missing values. This is the easiest approach and the one we will be using in this notebook. The immediate drawback of going with this approach is that you lose some information, and in some cases too much of it. Now imagine that half of the records have at least one or more missing values. Here you are presented with the tough decision of quantity vs quality. In any event, this decision must be made carefully, hence the reason for emphasizing it here in this notebook. \n",
    "\n",
    "- **Estimate Missing Values** - Here we try to estimate the missing values based on some criteria. Although this approach may be proven to be effective, it is not always the case, especially when we are dealing with sensitive data, like **Gender** or **Names**. For fields like **Address**, there could be ways to obtain these missing addresses using some data aggregation technique or obtain the information directly from other databases or public data sources.\n",
    "\n",
    "- **Ignore the missing value during analysis** - Here we basically ignore the missing values and proceed with our analysis. Although this is the most naive way to handle missing values it may proof effective, especially when the missing values includes information that is not important to the analysis being conducted. But think about it for a while. Would you ignore missing values, especially when in this day and age it is difficult to obtain high quality datasets? Again, there are some tradeoffs, which we will talk about later in the notebook.\n",
    "\n",
    "- **Replace with all possible values** - As an efficient and responsible data miner, we sometimes just need to put in the hard hours of work and find ways to makes up for these missing values. This last option is a very wise option for cases where data is scarce (which is almost always) or when dealing with sensitive data. Imagine that our dataset has an **Age** field, which contains many missing values. Since **Age** is a continuous variable, it means that we can build a separate model for calculating the age for the incomplete records based on some rule-based appraoch or probabilistic approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we are going to go with the first option but you may be asked to compute missing values, using a different approach, as an exercise. Let's get to it!\n",
    "\n",
    "First we want to add the dummy records with missing values since the dataset we have is perfectly composed and cleaned that it contains no missing values. First let us check for ourselves that indeed the dataset doesn't contain any missing values. We can do that easily by using the following built-in function provided by Pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       text  category  category_name\n0     False     False          False\n1     False     False          False\n2     False     False          False\n3     False     False          False\n4     False     False          False\n...     ...       ...            ...\n2252  False     False          False\n2253  False     False          False\n2254  False     False          False\n2255  False     False          False\n2256  False     False          False\n\n[2257 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>2252</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2253</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2254</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2255</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2256</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>2257 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `isnull` function looks through the entire dataset for null values and returns `True` wherever it finds any missing field or record. As you will see above, and as we anticipated, our dataset looks clean and all values are present, since `isnull` returns **False** for all fields and records. But let us start to get our hands dirty and build a nice little function to check each of the records, column by column, and return a nice little message telling us the amount of missing records found. This excerice will also encourage us to explore other capabilities of pandas dataframes. In most cases, the build-in functions are good enough, but as you saw above when the entire table was printed, it is impossible to tell if there are missing records just by looking at preview of records manually, especially in cases where the dataset is huge. We want a more reliable way to achieve this. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text             (The amoung of missing records is: , 0)\ncategory         (The amoung of missing records is: , 0)\ncategory_name    (The amoung of missing records is: , 0)\ndtype: object"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a lot happened there in that one line of code, so let's break it down. First, with the `isnull` we tranformed our table into the **True/False** table you see above, where **True** in this case means that the data is missing and **False** means that the data is present. We then take the transformed table and apply a function to each row that essentially counts to see if there are missing values in each record and print out how much missing values we found. In other words the `check_missing_values` function looks through each field (attribute or column) in the dataset and counts how many missing values were found. \n",
    "\n",
    "There are many other clever ways to check for missing data, and that is what makes Pandas so beautiful to work with. You get the control you need as a data scientist or just a person working in data mining projects. Indeed, Pandas makes your life easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 4 (5 min):** \n",
    "Let's try something different. Instead of calculating missing values by column let's try to calculate the missing values in every record instead of every column.  \n",
    "$Hint$ : `axis` parameter. Check the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our function to check for missing records, now let us do something mischievous and insert some dummy data into the dataframe and test the reliability of our function. This dummy data is intended to corrupt the dataset. I mean this happens a lot today, especially when hackers want to hijack or corrupt a database.\n",
    "\n",
    "We will insert a `Series`, which is basically a \"one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.\", into our current dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_series = pd.Series([\"dummy_record\", 1], index=[\"text\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text        dummy_record\ncategory               1\ndtype: object"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "dummy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_series = X.append(dummy_series, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2258"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# check if the records was commited into result\n",
    "len(result_with_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we that we have added the record with some missing values. Let try our function and see if it can detect that there is a missing value on the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text             (The amoung of missing records is: , 0)\ncategory         (The amoung of missing records is: , 0)\ncategory_name    (The amoung of missing records is: , 1)\ndtype: object"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there is a missing value in this new dataframe. Specifically, the missing value comes from the `category_name` attribute. As I mentioned before, there are many ways to conduct specific operations on the dataframes. In this case let us use a simple dictionary and try to insert it into our original dataframe `X`. Notice that above we are not changing the `X` dataframe as results are directly applied to the assignment variable provided. But in the event that we just want to keep things simple, we can just directly apply the changes to `X` and assign it to itself as we will do below. This modification will create a need to remove this dummy record later on, which means that we need to learn more about Pandas dataframes. This is getting intense! But just relax, everything will be fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy record as dictionary format\n",
    "dummy_dict = [{'text': 'dummy_record',\n",
    "               'category': 1\n",
    "              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(dummy_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2258"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text             (The amoung of missing records is: , 0)\ncategory         (The amoung of missing records is: , 0)\ncategory_name    (The amoung of missing records is: , 1)\ndtype: object"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we can see that our data has missing values, we want to remove the records with missing values. The code to drop the record with missing that we just added, is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now let us test to see if we gotten rid of the records with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text             (The amoung of missing records is: , 0)\ncategory         (The amoung of missing records is: , 0)\ncategory_name    (The amoung of missing records is: , 0)\ndtype: object"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2257"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are back with our original dataset, clean and tidy as we want it. That's enough on how to deal with missing values, let us now move unto something more fun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just in case you want to learn more about how to deal with missing data, refer to the official [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/missing_data.html#missing-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  id missing_example\n0  A             NaN\n1  B             NaN\n2  C             NaN\n3  D            None\n4  E            None\n5  F                ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>missing_example</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>A</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>B</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>C</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>D</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>E</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>F</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NA_dict = [{ 'id': 'A', 'missing_example': np.nan },\n",
    "           { 'id': 'B'                    },\n",
    "           { 'id': 'C', 'missing_example': 'NaN'  },\n",
    "           { 'id': 'D', 'missing_example': 'None' },\n",
    "           { 'id': 'E', 'missing_example':  None  },\n",
    "           { 'id': 'F', 'missing_example': ''     }]\n",
    "\n",
    "NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])\n",
    "NA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0     True\n1     True\n2    False\n3    False\n4     True\n5    False\nName: missing_example, dtype: bool"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "NA_df['missing_example'].isnull()"
   ]
  },
  {
   "source": [
    "# Answer here\n",
    "Let's start with the easiest, C and D both have a string (NaN and None respectively) as its value for 'missing_example', thus they both have value, and isnull() is working as intended here.\n",
    "\n",
    "Then let's look at B, which doesn't even have the value thus obviously isnull() returns true, same logic for A, even though there is an entry for 'missing_example', but its value is np.nan, which qualifies it for isnull according to documentation \"This function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).\" \n",
    "\n",
    "From the above documentation we can also see that E is working as intended, since None is also considered to be missing value for isnull()\n",
    "\n",
    "And for F, the value for 'missing_example' is an empty string. It still have a string as its value, but it just so happens that the content of the string is empty, thus it's not null, or, isnull() false."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with Duplicate Data\n",
    "Dealing with duplicate data is just as painful as dealing with missing data. The worst case is that you have duplicate data that has missing values. But let us not get carried away. Let us stick with the basics. As we have learned in our Data Mining course, duplicate data can occur because of many reasons. The majority of the times it has to do with how we store data or how we collect and merge data. For instance, we may have collected and stored a tweet, and a retweet of that same tweet as two different records; this results in a case of data duplication; the only difference being that one is the original tweet and the other the retweeted one. Here you will learn that dealing with duplicate data is not as challenging as missing values. But this also all depends on what you consider as duplicate data, i.e., this all depends on your criteria for what is considered as a duplicate record and also what type of data you are dealing with. For textual data, it may not be so trivial as it is for numerical values or images. Anyhow, let us look at some code on how to deal with duplicate records in our `X` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us check how many duplicates we have in our current dataset. Here is the line of code that checks for duplicates; it is very similar to the `isnull` function that we used to check for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n2252    False\n2253    False\n2254    False\n2255    False\n2256    False\nLength: 2257, dtype: bool"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "X.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the sum of duplicate records by simply doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that output, you may be asking why did the `duplicated` operation only returned one single column that indicates whether there is a duplicate record or not. So yes, all the `duplicated()` operation does is to check per records instead of per column. That is why the operation only returns one value instead of three values for each column. It appears that we don't have any duplicates since none of our records resulted in `True`. If we want to check for duplicates as we did above for some particular column, instead of all columns, we do something as shown below. As you may have noticed, in the case where we select some columns instead of checking by all columns, we are kind of lowering the criteria of what is considered as a duplicate record. So let us only check for duplicates by onyl checking the `text` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create some duplicated dummy records and append it to the main dataframe `X`. Subsequenlty, let us try to get rid of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_duplicate_dict = [{\n",
    "                             'text': 'dummy record',\n",
    "                             'category': 1, \n",
    "                             'category_name': \"dummy category\"\n",
    "                        },\n",
    "                        {\n",
    "                             'text': 'dummy record',\n",
    "                             'category': 1, \n",
    "                             'category_name': \"dummy category\"\n",
    "                        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(dummy_duplicate_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2259"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added the dummy duplicates to `X`. Now we are faced with the decision as to what to do with the duplicated records after we have found it. In our case, we want to get rid of all the duplicated records without preserving a copy. We can simply do that with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop_duplicates(keep=False, inplace=True) # inplace applies changes directly on our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2257"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the Pandas [documentation](http://pandas.pydata.org/pandas-docs/stable/indexing.html?highlight=duplicate#duplicate-data) for more information on dealing with duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Data Preprocessing\n",
    "In the Data Mining course we learned about the many ways of performing data preprocessing. In reality, the list is quiet general as the specifics of what data preprocessing involves is too much to cover in one course. This is especially true when you are dealing with unstructured data, as we are dealing with in this particular notebook. But let us look at some examples for each data preprocessing technique that we learned in the class. We will cover each item one by one, and provide example code for each category. You will learn how to peform each of the operations, using Pandas, that cover the essentials to Preprocessing in Data Mining. We are not going to follow any strict order, but the items we will cover in the preprocessing section of this notebook are as follows:\n",
    "\n",
    "- Aggregation\n",
    "- Sampling\n",
    "- Dimensionality Reduction\n",
    "- Feature Subset Selection\n",
    "- Feature Creation\n",
    "- Discretization and Binarization\n",
    "- Attribute Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sampling\n",
    "The first concept that we are going to cover from the above list is sampling. Sampling refers to the technique used for selecting data. The functionalities that we use to  selected data through queries provided by Pandas are actually basic methods for sampling. The reasons for sampling are sometimes due to the size of data -- we want a smaller subset of the data that is still representatitive enough as compared to the original dataset. \n",
    "\n",
    "We don't have a problem of size in our current dataset since it is just a couple thousand records long. But if we pay attention to how much content is included in the `text` field of each of those records, you will realize that sampling may not be a bad idea after all. In fact, we have already done some sampling by just reducing the records we are using here in this notebook; remember that we are only using four categories from the all the 20 categories available. Let us get an idea on how to sample using pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X.sample(n=1000) #random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1000"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "len(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   text  category  \\\n423   From: mccullou@snake2.cs.wisc.edu (Mark McCull...         0   \n1332  From: dougb@comm.mot.com (Doug Bank) Subject: ...         2   \n2197  From: keith@cco.caltech.edu (Keith Allan Schne...         0   \n514   From: des@helix.nih.gov (David E. Scheim) Subj...         2   \n\n     category_name  \n423    alt.atheism  \n1332       sci.med  \n2197   alt.atheism  \n514        sci.med  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>423</td>\n      <td>From: mccullou@snake2.cs.wisc.edu (Mark McCull...</td>\n      <td>0</td>\n      <td>alt.atheism</td>\n    </tr>\n    <tr>\n      <td>1332</td>\n      <td>From: dougb@comm.mot.com (Doug Bank) Subject: ...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <td>2197</td>\n      <td>From: keith@cco.caltech.edu (Keith Allan Schne...</td>\n      <td>0</td>\n      <td>alt.atheism</td>\n    </tr>\n    <tr>\n      <td>514</td>\n      <td>From: des@helix.nih.gov (David E. Scheim) Subj...</td>\n      <td>2</td>\n      <td>sci.med</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "X_sample[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes to the `X` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something cool here while we are working with sampling! Let us look at the distribution of categories in both the sample and original dataset. Let us visualize and analyze the disparity between the two datasets. To generate some visualizations, we are going to use `matplotlib` python library. With matplotlib, things are faster and compatability-wise it may just be the best visualization library for visualizing content extracted from dataframes and when using Jupyter notebooks. Let's take a loot at the magic of `matplotlib` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;alt.atheism&#39;, &#39;soc.religion.christian&#39;, &#39;comp.graphics&#39;, &#39;sci.med&#39;]"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "soc.religion.christian    599\nsci.med                   594\ncomp.graphics             584\nalt.atheism               480\nName: category_name, dtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;matplotlib.axes._subplots.AxesSubplot at 0x191aa3d8688&gt;"
     },
     "metadata": {},
     "execution_count": 64
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;Figure size 576x216 with 1 Axes&gt;",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"210.244063pt\" version=\"1.1\" viewBox=\"0 0 488.79625 210.244063\" width=\"488.79625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 210.244063 \r\nL 488.79625 210.244063 \r\nL 488.79625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 35.19625 185.398125 \r\nL 481.59625 185.398125 \r\nL 481.59625 22.318125 \r\nL 35.19625 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p98c69dc630)\" d=\"M 63.09625 185.398125 \r\nL 118.89625 185.398125 \r\nL 118.89625 35.113633 \r\nL 63.09625 35.113633 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p98c69dc630)\" d=\"M 174.69625 185.398125 \r\nL 230.49625 185.398125 \r\nL 230.49625 36.368094 \r\nL 174.69625 36.368094 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p98c69dc630)\" d=\"M 286.29625 185.398125 \r\nL 342.09625 185.398125 \r\nL 342.09625 38.877017 \r\nL 286.29625 38.877017 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p98c69dc630)\" d=\"M 397.89625 185.398125 \r\nL 453.69625 185.398125 \r\nL 453.69625 64.969817 \r\nL 397.89625 64.969817 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m041503c206\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"90.99625\" xlink:href=\"#m041503c206\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- soc.religion.christian -->\r\n      <defs>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      </defs>\r\n      <g transform=\"translate(33.971563 200.756406)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"113.28125\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"168.261719\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"200.048828\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"241.130859\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"302.654297\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"330.4375\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"358.220703\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"421.697266\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"449.480469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"510.662109\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"574.041016\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"605.828125\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"660.808594\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"724.1875\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"765.300781\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"793.083984\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"845.183594\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"884.392578\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"912.175781\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"973.455078\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.59625\" xlink:href=\"#m041503c206\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- sci.med -->\r\n      <defs>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <g transform=\"translate(181.197812 200.756406)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"107.080078\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"134.863281\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"166.650391\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"264.0625\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"325.585938\" xlink:href=\"#DejaVuSans-100\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.19625\" xlink:href=\"#m041503c206\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- comp.graphics -->\r\n      <defs>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      </defs>\r\n      <g transform=\"translate(273.692187 200.756406)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"213.574219\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"277.050781\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"308.837891\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"372.314453\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"413.427734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"474.707031\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"538.183594\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"601.5625\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"629.345703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"684.326172\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"425.79625\" xlink:href=\"#m041503c206\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- alt.atheism -->\r\n      <g transform=\"translate(394.846719 200.756406)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"128.271484\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"160.058594\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"221.337891\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"260.546875\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"323.925781\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"385.449219\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"413.232422\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"465.332031\" xlink:href=\"#DejaVuSans-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_5\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0339ff241c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(21.1975 189.577266)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"160.308894\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 164.488035)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"135.219663\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 139.398804)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"110.130433\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 114.309573)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"85.041202\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 89.220343)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"59.951971\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 500 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 64.131112)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.19625\" xlink:href=\"#m0339ff241c\" y=\"34.86274\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 600 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 39.041881)scale(0.11 -0.11)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 35.19625 185.398125 \r\nL 35.19625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 481.59625 185.398125 \r\nL 481.59625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 35.19625 185.398125 \r\nL 481.59625 185.398125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 35.19625 22.318125 \r\nL 481.59625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- Category distribution -->\r\n    <defs>\r\n     <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n     <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n    </defs>\r\n    <g transform=\"translate(194.889063 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"131.103516\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"170.3125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"231.835938\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"295.3125\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"356.494141\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"397.607422\" xlink:href=\"#DejaVuSans-121\"/>\r\n     <use x=\"456.787109\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"488.574219\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"552.050781\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"579.833984\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"631.933594\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"671.142578\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"712.255859\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"740.039062\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"803.515625\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"866.894531\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"906.103516\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"933.886719\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"995.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p98c69dc630\">\r\n   <rect height=\"163.08\" width=\"446.4\" x=\"35.19625\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(X.category_name.value_counts())\n",
    "\n",
    "# plot barchart for X_sample\n",
    "X.category_name.value_counts().plot(kind = 'bar',\n",
    "                                    title = 'Category distribution',\n",
    "                                    ylim = [0, 650],        \n",
    "                                    rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sci.med                   272\ncomp.graphics             260\nsoc.religion.christian    253\nalt.atheism               215\nName: category_name, dtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;matplotlib.axes._subplots.AxesSubplot at 0x191aa49a6c8&gt;"
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;Figure size 576x216 with 1 Axes&gt;",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"211.211875pt\" version=\"1.1\" viewBox=\"0 0 490.705 211.211875\" width=\"490.705pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 211.211875 \r\nL 490.705 211.211875 \r\nL 490.705 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 37.105 185.398125 \r\nL 483.505 185.398125 \r\nL 483.505 22.318125 \r\nL 37.105 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p464ac6088c)\" d=\"M 65.005 185.398125 \r\nL 120.805 185.398125 \r\nL 120.805 37.538925 \r\nL 65.005 37.538925 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p464ac6088c)\" d=\"M 176.605 185.398125 \r\nL 232.405 185.398125 \r\nL 232.405 44.062125 \r\nL 176.605 44.062125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p464ac6088c)\" d=\"M 288.205 185.398125 \r\nL 344.005 185.398125 \r\nL 344.005 47.867325 \r\nL 288.205 47.867325 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p464ac6088c)\" d=\"M 399.805 185.398125 \r\nL 455.605 185.398125 \r\nL 455.605 68.524125 \r\nL 399.805 68.524125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"md8e6b0512a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.905\" xlink:href=\"#md8e6b0512a\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- sci.med -->\r\n      <defs>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <g transform=\"translate(69.56125 201.51625)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"107.080078\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"134.863281\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"166.650391\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"264.0625\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"325.585938\" xlink:href=\"#DejaVuSans-100\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.505\" xlink:href=\"#md8e6b0512a\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- comp.graphics -->\r\n      <defs>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <g transform=\"translate(160.31875 201.51625)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"213.574219\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"277.050781\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"308.837891\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"372.314453\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"413.427734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"474.707031\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"538.183594\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"601.5625\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"629.345703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"684.326172\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"316.105\" xlink:href=\"#md8e6b0512a\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- soc.religion.christian -->\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <g transform=\"translate(253.89625 201.51625)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"113.28125\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"168.261719\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"200.048828\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"241.130859\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"302.654297\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"330.4375\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"358.220703\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"421.697266\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"449.480469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"510.662109\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"574.041016\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"605.828125\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"660.808594\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"724.1875\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"765.300781\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"793.083984\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"845.183594\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"884.392578\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"912.175781\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"973.455078\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"427.705\" xlink:href=\"#md8e6b0512a\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- alt.atheism -->\r\n      <g transform=\"translate(393.941875 201.51625)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"128.271484\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"160.058594\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"221.337891\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"260.546875\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"323.925781\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"385.449219\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"413.232422\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"465.332031\" xlink:href=\"#DejaVuSans-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_5\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m86dbfec52f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"185.398125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(22.47 189.957188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"158.218125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(14.835 162.777188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"131.038125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 135.597188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"103.858125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(7.2 108.417188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"76.678125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 81.237188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"49.498125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(7.2 54.057187)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.105\" xlink:href=\"#m86dbfec52f\" y=\"22.318125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 26.877188)scale(0.12 -0.12)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 37.105 185.398125 \r\nL 37.105 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 483.505 185.398125 \r\nL 483.505 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 37.105 185.398125 \r\nL 483.505 185.398125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 37.105 22.318125 \r\nL 483.505 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- Category distribution -->\r\n    <defs>\r\n     <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n     <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n    </defs>\r\n    <g transform=\"translate(196.797813 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"131.103516\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"170.3125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"231.835938\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"295.3125\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"356.494141\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"397.607422\" xlink:href=\"#DejaVuSans-121\"/>\r\n     <use x=\"456.787109\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"488.574219\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"552.050781\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"579.833984\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"631.933594\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"671.142578\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"712.255859\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"740.039062\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"803.515625\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"866.894531\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"906.103516\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"933.886719\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"995.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p464ac6088c\">\r\n   <rect height=\"163.08\" width=\"446.4\" x=\"37.105\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAADVCAYAAACYNrP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeqElEQVR4nO3de5wcVZ338c83JCaYiwnJAJsIYcGQYNCwEg3sImRlBcUHYUWNEsSAgKi4Il6XJ0hWUHjUFV4uXgiCAbmIPAZWYGWV5eKiiE9QwxoMapQhkAsTyG0ChIu/549zGmo63TM9mZ5Mzcz3/XrNa7rqnDp1TlV1/7pOnapWRGBmZmblNKSvK2BmZmb1OVCbmZmVmAO1mZlZiTlQm5mZlZgDtZmZWYk5UJuZmZWYA7WZvUjSXZJOya/nSvpxE8teJml2fr1A0tVNLPtsSd9uVnlmZeJAbVaDpOMlLZHULmm1pB9JOqTBZUPSq3q7jr0tIq6JiCO6yidpkaTzGyhvekTc1dN6SZot6dGqsr8YEaf0tGyzMnKgNqsi6SzgYuCLwG7AnsA3gGP6sl5dkTS0r+tQS1nrZdZfOFCbFUh6BfB54CMRsTgitkTEcxFxc0R8Kud5g6R7JW3IZ9uXSHpZTvtpLmppPhufk+f/L0m/ycv8XNJrC+t8naRfS9os6QZJ1xfPUCWdKumPkp6U9ENJEwtpIekjkv4A/EHS1yX9a1WbbpZ0Zp32vlnSckkbJV0CqJA2T9I9+bUkXSTp8Zz3AUn7SzoNmAt8Orf35pz/YUmfkfQAsEXS0DzvHwqrH5HbulnSryTNqGrXqwrTiySdL2kk8CNgYl5fu6SJ1V3pkt6eu9o35O78/QppD0v6ZG7DxlyHETUPCLMScKA26+hgYARwYyd5XgA+DkzI+Q8HPgwQEYfmPDMiYlREXC/pdcAVwAeB8cClwA8lDc8B/kZgEbALcB3wj5UVSXoTcAHwbuCvgFbge1X1ORaYBbwauBJ4r6QhefkJuX7XVTcip/0AmJ/bsgL4uzptPgI4FNgXGAvMAZ6IiIXANcCXcnuPLizzXuBtwNiIeL5GmccAN+R2XwvcJGlYnfUDEBFbgLcCq/L6RkXEqqp27ZvbeybQAvwHcHPly1T2buAtwF8DrwXmdbZes77kQG3W0XhgXZ3AAkBE3B8Rv4iI5yPiYVLgPayTMk8FLo2I+yLihYi4EtgKHJT/hgJfy2fui4FfFpadC1wREb+KiK3APwMHS9qrkOeCiHgyIp6OiF8CG0nBGeA9wF0RsbZGvY4CHoyI/xsRz5G6+9fUacNzwGhgGqCI+F1ErO6kzeQ2rYyIp+uk319Y91dJX5AO6qLMRswBbo2In+SyvwLsDPxtVd1WRcSTwM3AAU1Yr1mvcKA26+gJYEJn11Ul7SvpFklrJG0iXcue0EmZk4FP5G7YDZI2AHsAE/PfY9Hx13FWFl5PJJ1FAxAR7bmOk+rkh3RWfUJ+fQLw3Tr1mlhcNtehuqxK2h3AJcDXgbWSFkoaU6fcevWqmx4RfwEezXXqqept9pe8ruI2K34heQoY1YT1mvUKB2qzju4FniF1J9fzTWA5MCUixgBnU7i2W8NK4AsRMbbw9/KIuA5YDUySVFx+j8LrVaRAD0C+RjseeKyQp/on8K4GjsnXfPcDbqpTr9XFdeU67FEnLxHxtYg4EJhO6gL/VJ3116tXteK6hwCvJLUXUvB8eSHv7t0ot3qbVdr1WN0lzErMgdqsICI2Ap8Dvi7pWEkvlzRM0lslfSlnGw1sAtolTQM+VFXMWmDvwvRlwOmSZuVBWSMlvU3SaNIXgxeAM/KAq2OANxSWvRY4SdIBkoaTzt7vy13u9drwKPD/SGfSP+ik6/lWYLqkd+QehH+iY0B8kaTX5/oPA7aQvsy8UKe9jTqwsO4zSZcDfpHTfgMcL2knSW+h46WFtcB4pYF/tXwfeJukw3N9P5HL/vl21NGszzlQm1WJiK8CZ5EGWbWRzojP4KUz008CxwObSUH4+qoiFgBX5m7ud0fEEtJ16kuA9cAfyYOXIuJZ4B3AB4ANpK7qW0iBhYj4L+Ac0qCv1cA+pOvOXbkSeA31u72JiHXAu4ALSd3pU4Cf1ck+Jrd1Palb+QnStV+Ay4FX5/bWO3uv5d9J15PXA+8D3pGvKQN8DDiatE3mUugViIjlpMFif8rr7NBdHhEPkbbjvwHrcjlH521t1u+o46UxM+trku4DvhUR3+lBGYeSusD3ytdozayf8hm1WR+TdJik3XPX9/tJtwvd1oPyhpHOSL/tIG3W/zUUqCVdrfRgh02Sfq/8LOCcdrjSAxOeknSnpOIgjuGSrsjLrVF64pOZdTQVWEq6reoTwDsbuPWppvxgjw2ke64vbloNzazPNNT1LWk68MeI2JoHz9xFepBBK+khCaeQ7kU8D3hjRByUl7sAOAR4O2mQyp3AvIjY7rMFMzOzwaTb16glTSUF6o+RnlA0LyL+NqeNJA3e+JuIWC7pMeCkiPhxTj+PdEtLI4NhzMzMBr2Gr1FL+oakp0j3j64mPZZvOqnLDnjx8X4rSLd8jCM9eGBpoZileRkzMzNrQMO/ahMRH5b0UdKzjWeTbh8ZRbp9pWgj6T7TUYXp6rRtKD3c/zSAkSNHHjht2rRGq2ZmZtav3X///esioqVWWrd+fi4iXgDukXQC6SEP7aT7K4vGkO4vbS9MP1OVVqvshcBCgJkzZ8aSJUu6UzUzM7N+S1JrvbTtvT1rKOnBC8uA4k/TjazMj4j1pC7yGYXlZuRlzMzMrAFdBmpJu0p6j6RR+XF+R5J+vu4O0s/z7S/puPx7rp8DHshPDgK4CpgvaVweLX4q6ef8zMzMrAGNnFEHqZv7UdKj/r4CnBkR/x4RbcBxwBdy2iw6Pt7wXNLgslbgbuDLvjXLzMyscV1eo87BuO5v7UbE7aTfqK2VthU4Of+ZmZlZN/kRomZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl1q1fzxqo9vrsrX1dhV718IVv6+sqmJnZdvIZtZmZWYk5UJuZmZWYA7WZmVmJOVCbmZmVmAO1mZlZiXnUt/VrHrFvZgOdz6jNzMxKrMtALWm4pMsltUraLOnXkt6a0/aSFJLaC3/nVC17haRNktZIOqs3G2NmZjbQNNL1PRRYCRwGPAIcBXxf0msKecZGxPM1ll0ATAEmA7sDd0p6MCJu61GtzczMBokuA3VEbCEF3IpbJP0ZOBC4v4vFTwROioj1wHpJlwHzAAdqM/MYA7MGdPsataTdgH2BZYXZrZIelfQdSRNyvnHARGBpId9SYHoP6mtmZjaodCtQSxoGXANcGRHLgXXA60ld2wcCo3M6wKj8f2OhiI05T62yT5O0RNKStra27lTLzMxswGo4UEsaAnwXeBY4AyAi2iNiSUQ8HxFr8/wjJI0B2vOiYwrFjAE21yo/IhZGxMyImNnS0rIdTTEzMxt4GgrUkgRcDuwGHBcRz9XJGpVF8nXp1cCMQvoMOnaZm5mZWScaPaP+JrAfcHREPF2ZKWmWpKmShkgaD3wNuCsiKt3dVwHzJY2TNA04FVjUvOqbmZkNbI3cRz0Z+CBwALCmcL/0XGBv0gjuzcBvga3AewuLnwusAFqBu4Ev+9YsMzOzxjVye1YroE6yXNfJsluBk/OfmZmZdZMfIWpmZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiXf56lpmZWS17ffbWvq5Cr3n4wrf1dRVe5DNqMzOzEnOgNjMzKzEHajMzsxJzoDYzMyuxLgO1pOGSLpfUKmmzpF9Lemsh/XBJyyU9JelOSZOrlr1C0iZJaySd1VsNMTMzG4gaOaMeCqwEDgNeAZwDfF/SXpImAIvzvF2AJcD1hWUXAFOAycDfA5+W9Jam1d7MzGyA6/L2rIjYQgq4FbdI+jNwIDAeWBYRNwBIWgCskzQtIpYDJwInRcR6YL2ky4B5wG3NbISZmdlA1e1r1JJ2A/YFlgHTgaWVtBzUVwDTJY0DJhbT8+vpdco9TdISSUva2tq6Wy0zM7MBqVuBWtIw4BrgynzGPArYWJVtIzA6p1GVXknbRkQsjIiZETGzpaWlO9UyMzMbsBoO1JKGAN8FngXOyLPbgTFVWccAm3MaVemVNDMzM2tAQ4FakoDLgd2A4yLiuZy0DJhRyDcS2Id03Xo9sLqYnl8va0K9zczMBoVGz6i/CewHHB0RTxfm3wjsL+k4SSOAzwEP5G5xgKuA+ZLGSZoGnAosak7VzczMBr5G7qOeDHwQOABYI6k9/82NiDbgOOALwHpgFvCewuLnkgaXtQJ3A1+OCI/4NjMza1Ajt2e1Auok/XZgWp20rcDJ+c/MzMy6yY8QNTMzKzEHajMzsxJzoDYzMysxB2ozM7MSc6A2MzMrMQdqMzOzEnOgNjMzKzEHajMzsxJzoDYzMysxB2ozM7MSc6A2MzMrMQdqMzOzEnOgNjMzKzEHajMzsxJzoDYzMysxB2ozM7MSc6A2MzMrsYYCtaQzJC2RtFXSosL8vSSFpPbC3zmF9OGSrpC0SdIaSWf1QhvMzMwGrKEN5lsFnA8cCexcI31sRDxfY/4CYAowGdgduFPSgxFx23bU1czMbNBp6Iw6IhZHxE3AE90s/0TgvIhYHxG/Ay4D5nWzDDMzs0GrWdeoWyU9Kuk7kiYASBoHTASWFvItBabXKkDSabl7fUlbW1uTqmVmZta/9TRQrwNeT+raPhAYDVyT00bl/xsL+TfmPNuIiIURMTMiZra0tPSwWmZmZgNDo9eoa4qIdmBJnlwr6QxgtaQxQHuePwZ4pvB6c0/WaWZmNpg0+/asyP8VEeuB1cCMQvoMYFmT12lmZjZgNXp71lBJI4CdgJ0kjcjzZkmaKmmIpPHA14C7IqLS3X0VMF/SOEnTgFOBRb3QDjMzswGp0TPq+cDTwGeBE/Lr+cDewG2k7uzfAluB9xaWOxdYAbQCdwNf9q1ZZmZmjWvoGnVELCDdE13LdZ0stxU4Of+ZmZlZN/kRomZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJdZQoJZ0hqQlkrZKWlSVdrik5ZKeknSnpMmFtOGSrpC0SdIaSWc1uf5mZmYDWqNn1KuA84ErijMlTQAWA+cAuwBLgOsLWRYAU4DJwN8Dn5b0lp5V2czMbPBoKFBHxOKIuAl4oirpHcCyiLghIp4hBeYZkqbl9BOB8yJifUT8DrgMmNeUmpuZmQ0CPb1GPR1YWpmIiC3ACmC6pHHAxGJ6fj29VkGSTsvd60va2tp6WC0zM7OBoaeBehSwsWreRmB0TqMqvZK2jYhYGBEzI2JmS0tLD6tlZmY2MPQ0ULcDY6rmjQE25zSq0itpZmZm1oCeBuplwIzKhKSRwD6k69brgdXF9Px6WQ/XaWZmNmg0envWUEkjgJ2AnSSNkDQUuBHYX9JxOf1zwAMRsTwvehUwX9K4PMDsVGBR01thZmY2QDV6Rj0feBr4LHBCfj0/ItqA44AvAOuBWcB7CsudSxpc1grcDXw5Im5rTtXNzMwGvqGNZIqIBaRbr2ql3Q5Mq5O2FTg5/5mZmVk3+RGiZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl1pRALekuSc9Ias9/DxXSjpfUKmmLpJsk7dKMdZqZmQ0GzTyjPiMiRuW/qQCSpgOXAu8DdgOeAr7RxHWamZkNaEN7ufy5wM0R8VMASecAv5M0OiI29/K6zczM+r1mnlFfIGmdpJ9Jmp3nTQeWVjJExArgWWDfJq7XzMxswGpWoP4MsDcwCVgI3CxpH2AUsLEq70ZgdHUBkk6TtETSkra2tiZVy8zMrH9rSqCOiPsiYnNEbI2IK4GfAUcB7cCYquxjgG26vSNiYUTMjIiZLS0tzaiWmZlZv9dbt2cFIGAZMKMyU9LewHDg9720XjMzswGlx4PJJI0FZgF3A88Dc4BDgTNz+fdKeiPwK+DzwGIPJDMzM2tMM0Z9DwPOB6YBLwDLgWMj4iEASacD1wDjgduBk5qwTjMzs0Ghx4E6ItqA13eSfi1wbU/XY2ZmNhj5EaJmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXmQG1mZlZiDtRmZmYl5kBtZmZWYg7UZmZmJeZAbWZmVmIO1GZmZiXW64Fa0i6SbpS0RVKrpON7e51mZmYDxdAdsI6vA88CuwEHALdKWhoRy3bAus3MzPq1Xj2jljQSOA44JyLaI+Ie4IfA+3pzvWZmZgNFb3d97wu8EBG/L8xbCkzv5fWamZkNCIqI3itceiNwQ0TsXph3KjA3ImZX5T0NOC1PTgUe6rWK9b0JwLq+roRtF++7/s37r/8a6PtuckS01Ero7WvU7cCYqnljgM3VGSNiIbCwl+tTCpKWRMTMvq6HdZ/3Xf/m/dd/DeZ919td378HhkqaUpg3A/BAMjMzswb0aqCOiC3AYuDzkkZK+jvgGOC7vbleMzOzgWJHPPDkw8DOwOPAdcCHfGvW4OjiH6C87/o377/+a9Duu14dTGZmZmY940eImpmZlZgDdRNJape0dx+sd4Gkq3f0eq05JM2W9Ggn6d+SdM6OrNOOJmmRpPPz6zdKauj2zO7kbRZJd0k6pRv598yfDTttx7pKv+8lzZN0Ty+W3+n7o4tld/jx0Rt2xCNEB42IGNXXdbCBJyJO7+s67EgR8d+kZyk0NW9fiYhHgC4/GyTNA06JiEMKy/a7fS8pgCkR8ce+WL6oPxwfjfAZtVkPSfIX3k4M5u0zmNtuzeNA3QlJn5H0mKTNkh6SdLiknSSdLWlFnn+/pD1y/pD0qjplPSzpU5IeyL8kdrmk3ST9KJdzu6RxhfwHSfq5pA2SlkqaXUj7a0l35+V+QnpiT78maQ9JiyW1SXpC0iWShkian3917XFJV0l6Rc6/V97eJ0laKWm9pNMlvT5v4w2SLimUP0/SzyT9m6SNkpZLOryT+hyR9/lGSd/I2/uUqrIukvQksEDSPpLuyHVfJ+kaSWML5T0s6Z8lPZjr+h1JI6rW+YncztWSTirMf7FbOE8fI+k3kjbl4/AthXr9KR8Xf5Y0t4ttXuv4Hi7pYkmr8t/FkoZ3te4aZYekj0j6A/CHPG+apJ9IejKv7911lu3Q1SnpdZJ+net5g6Tr9VI3eXXe/ZS6pjdIWibp7VXb8euSbs1l3Sdpn062T2dtnZyPgc2SfixpQl6mclx+QNIjwB2FeUPr7SdJ+wHfAg5W6ibfUKhzpa3jJN2i9B5Zn1+/slDfuySdV6tezSDps3rpc+9BSf9YI89P88uluR1zauR5g6R78z5arfRef1lXy3fy/hgu6SuSHpG0Vulywc45rfr42OaYz/MX5GPr6pz2P5L2VXrPPq70GXNEM7bjdokI/9X4I3WXrAQm5um9gH2ATwH/k9NFeoDL+JwngFfVKe9h4BekXxGbRLpd7VfA3wDDgTuAc3PeScATwFGkL1NvztMtOf1e4Kt5uUNJT3q7uq+3WQ+29U6kZ8BfBIwERgCHACcDfwT2JnUdLga+W9gfQfpwGwEcATwD3ATsWtjGh+X884DngY8Dw4A5wEZglxr1mQBsAt5Bujz0MeA5UrdksayP5vSdgVfl/TQcaAF+Clxctf9/C+wB7AL8DDg/p83O5X0+1+0o4ClgXE5fVMj7hlzvN+djYxIwLW+3TcDUnO+vgOnbcXx/Ph+nu+Z2/Bw4r7N11yk/gJ/ktu6c67cSOClvs9eRHgc5vUYbZwOP5tcvA1rzPhiW98mzdfIOIx0vZ+fl3kR6b0wtrOPJ3I6hwDXA9+rUv25bgbuAFaTfMtg5T19YdVxeldu8c2He0M72E+m4uqeqHsXtMp70I0cvB0YDNwA3FfLWrVeT3qfvAibm7TEH2JLr36HedPI5mNMPBA7K22Mv4HfAmfWWp+v3x8WkH3vaJW+Xm4ELahwfNY/5/HoB6fPjyFyvq4A/A/87r/NU4M999hnZVysu+x/pg/dx4B+AYYX5DwHH1Fmmq0A9tzD9A+CbhemPVt50wGfIAamQ/p/A+4E980E7spB2Lf07UB8MtAFDq+b/F/DhwvRUUsCsvMEDmFRIfwKYU7WNz8yv5wGryLck5nm/BN5Xoz4nAvcWppXf4MVA/UgXbToW+HXV/j+9MH0UsCK/ng08XWx/PvYOyq8X8dKH9aXARTXWNxLYQPog37kHx/cK4KjC9JHAw52tu5P3wpsK03OA/67KcykvfTkttnE2L324Hgo8VrXf7qmT943AGmBIIe91wILCOr5dtQ+W16l/3baSAuD8wvSHgdvy68pxuXchvTKvEqhr7ie6CNQ16nEAsL6RevXGH/Ab0gOsOtSbLgJ1jXLOBG6stzydvD9I780t5ICb0w4mB9Wq46PmMZ/TFgA/KUwfTXoE9k55enSu19je2p6d/bnru45IAxnOJO3AxyV9T9JE0hnRiu0sdm3h9dM1pisDTiYD78pdQxtyN9ghpG+vE0lvzi2FZVu3sz5lsQfQGhHPV82fSMe2tZI+7HYrzGt0mwI8FvldVyhvYo36TCQFZgDyMtWjTlcWJyTtmo+RxyRtAq5m20sSxWWq1/1EVfufovYApJrHXz4e5gCnA6tz9+60GstX8tc7vmtt80o9u3vsF9s7GZhVdUzPBXavveiLJrLtflvZSd6VEfGXwrxW0tlwxZrC63rbGLpua1fl1Kxjd/dTkaSXS7pU6VLQJlKvzVh1HE3eaPu6TdKJ+VJAZf/tz3ZcdstdyrdIWpPb8cUGyqn3/mgh9TDcX6jXbXl+B50c8xXVnx3rIuKFwjQ0cXt2hwN1JyLi2kgjMCeTvk39H9IbsO51rSZZSTqjHlv4GxkRFwKrgXFKv/VdsWcv16e3rQT21LYDb1aRtn1FpTdhLdtnkiRVlbeqRr7VQPHan4rTWVRNX5DnvTYixgAnkL7tF+3RwLq7Uvf4i4j/jIg3k77QLQcu66ygOsd3rW1eqWd3j/3q4Hp31TE9KiI+1EUZq9l2v+1RJ+8qYA9Jxc+1PUln5N3V0/d59fHxUkL9/VR3mewTpF6lWfkYOzTPrz7Omk7SZFI9zyBd6htLupSzPev+JqndU3I7zt7OciBdPnmadPmgcly9IurcgVPnmC89B+o6JE2V9CalgTTPkA6GF4BvA+dJmqLktZLGN3n1VwNHSzpSafDaiDwo4pUR0QosAf5F0sskHULqpunPfkn6QL5Q6ZnwI5SeC38d8HGlwXOjSN+8r69x5t2oXYF/kjRM0ruA/YD/qJHvVuA1ko7NXx4+QtdnfqNJXWUbJE0ijWWo9hFJr5S0C+nD6frtaMPlwElKA7+GSJqkNEhrN0lvz1/gtua6vFCvkE6O7+uA+ZJalAYifY50PNZdd4P1vgXYV9L78vYfpjTwb78ulrs31+sMSUMlHUO6flzLfaRu0E/n8meT3hvfa7CORT1pa11d7Ke1wCuVB1bVMJq0nzbkY+jcntanG0aSAlsbgNJgrv3r5F1LGldSz2jSdfr2vE2rv6x1tfyLcu/JZcBFknbNdZsk6cjqvJ0c86XnQF3fcOBC0je2NaQP+bNJg7i+D/yYdLBdThq40YHSyPAfbc+KI2Il6drP2aQ3xkrSB39lfx0PzCINjDmXNPCh38rdS0eTriE9QupmngNcQfoBl5+SBnY8Q7qWv73uA6aQ9ukXgHdGxBPw4oMlvpXrs440cOZLpOveryZ9OdraSdn/QhogtZEU6BfXyHMt6bj5U/47v0aeTkXEL0kDsi7K67qbdHYwhHTGtYp0XBxGukZZT73j+3xSWx8gDZr8VaWenay7w/arU+/NpAF/78l1XEM6mxleb5m83LOkAWQfIF3bPYEU9LfZFznv24G35nZ9AzgxIpZ3to4KpVHic7tqaw91tp/uIP2y4BpJtX53+WLSZ8060oC/25pQn4ZExIPAv5K+OK0FXkMaEFnLAuDK3BX9br30wJdKz98nSZ9hm0lBtvoLa4flG6jeZ0iDCH+Ru9Jvp/a90/WO+dLzs75tUFCNh0l0Y9khpC8PcyPizu1c/8N5/bdvz/L2Ekn3Ad+KiO/0dV3MdgSfUZvVkC87jM3dZJVraL/o42oNSpIOk7R77vp+P/BaduDZpFlf81NzzGo7mNRV/TLgQeDYiHi680Wsl0wlXW4aRRqJ/c6IWN23VTLbcdz1bWZmVmLu+jYzMysxB2ozM7MSc6A2MzMrMQdqMzOzEnOgNjMzKzEHajMzsxL7/9E0q7530T0EAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(X_sample.category_name.value_counts())\n",
    "\n",
    "# plot barchart for X_sample\n",
    "X_sample.category_name.value_counts().plot(kind = 'bar',\n",
    "                                           title = 'Category distribution',\n",
    "                                           ylim = [0, 300], \n",
    "                                           rot = 0, fontsize = 12, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use following command to see other available styles to prettify your charts.\n",
    "```python\n",
    "print(plt.style.available)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 7 (5 min):**\n",
    "Notice that for the `ylim` parameters we hardcoded the maximum value for y. Is it possible to automate this instead of hard-coding it? How would you go about doing that? (Hint: look at code above for clues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "# plot barchart for X_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://i.imgur.com/9eO431H.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that stood out from the both datasets, is that the distribution of the categories remain relatively the same, which is a good sign for us data scientist. There are many ways to conduct sampling on the dataset and still obtain a representative enough dataset. That is not the main focus in this notebook, but if you would like to know more about sampling and how the `sample` feature works, just reference the Pandas documentation and you will find interesting ways to conduct more advanced sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Creation\n",
    "The other operation from the list above that we are going to practise on is the so-called feature creation. As the name suggests, in feature creation we are looking at creating new interesting and useful features from the original dataset; a feature which captures the most important information from the raw information we already have access to. In our `X` table, we would like to create some features from the `text` field, but we are still not sure what kind of features we want to create. We can think of an interesting problem we want to solve, or something we want to analyze from the data, or some questions we want to answer. This is one process to come up with features -- this process is usually called `feature engineering` in the data science community. \n",
    "\n",
    "We know what feature creation is so let us get real involved with our dataset and make it more interesting by adding some special features or attributes if you will. First, we are going to obtain the **unigrams** for each text. (Unigram is just a fancy word we use in Text Mining which stands for 'tokens' or 'individual words'.) Yes, we want to extract all the words found in each text and append it as a new feature to the pandas dataframe. The reason for extracting unigrams is not so clear yet, but we can start to think of obtaining some statistics about the articles we have: something like **word distribution** or **word frequency**.\n",
    "\n",
    "Before going into any further coding, we will also introduce a useful text mining library called [NLTK](http://www.nltk.org/). The NLTK library is a natural language processing tool used for text mining tasks, so might as well we start to familiarize ourselves with it from now (It may come in handy for the final project!). In partcular, we are going to use the NLTK library to conduct tokenization because we are interested in splitting a sentence into its individual components, which we refer to as words, emojis, emails, etc. So let us go for it! We can call the `nltk` library as follows:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m&gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download(&#39;punkt&#39;)\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - &#39;C:\\\\Users\\\\Jerry/nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\share\\\\nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\lib\\\\nltk_data&#39;\n    - &#39;C:\\\\Users\\\\Jerry\\\\AppData\\\\Roaming\\\\nltk_data&#39;\n    - &#39;C:\\\\nltk_data&#39;\n    - &#39;D:\\\\nltk_data&#39;\n    - &#39;E:\\\\nltk_data&#39;\n    - &#39;&#39;\n**********************************************************************\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-69-6c190648024c&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# takes a like a minute or two to process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 2\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;unigrams&#39;\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;text&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdmh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-&gt; 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m&lt;ipython-input-69-6c190648024c&gt;\u001b[0m in \u001b[0;36m&lt;lambda&gt;\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# takes a like a minute or two to process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 2\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;unigrams&#39;\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;text&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdmh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Jerry\\Documents\\1091\\資料探勘\\Lab1\\DM2020-Lab1-Master\\helpers\\data_mining_helpers.py\u001b[0m in \u001b[0;36mtokenize_text\u001b[1;34m(text, remove_stopwords)\u001b[0m\n\u001b[0;32m     32\u001b[0m     &quot;&quot;&quot;\n\u001b[0;32m     33\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---&gt; 34\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m&#39;english&#39;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m&#39;english&#39;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# filters here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     &quot;&quot;&quot;\n\u001b[1;32m--&gt; 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;tokenizers/punkt/{0}.pickle&#39;\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m&#39;raw&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m&#39;nltk&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m&#39;&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m&#39;file&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode=&#39;rb&#39;, so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m&#39;*&#39;\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m&#39;\\n%s\\n%s\\n%s\\n&#39;\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m&gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download(&#39;punkt&#39;)\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - &#39;C:\\\\Users\\\\Jerry/nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\share\\\\nltk_data&#39;\n    - &#39;G:\\\\Anaconda3\\\\lib\\\\nltk_data&#39;\n    - &#39;C:\\\\Users\\\\Jerry\\\\AppData\\\\Roaming\\\\nltk_data&#39;\n    - &#39;C:\\\\nltk_data&#39;\n    - &#39;D:\\\\nltk_data&#39;\n    - &#39;E:\\\\nltk_data&#39;\n    - &#39;&#39;\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# takes a like a minute or two to process\n",
    "X['unigrams'] = X['text'].apply(lambda x: dmh.tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:4][\"unigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a closer look at the `X` table now, you will see the new columns `unigrams` that we have added. You will notice that it contains an array of tokens, which were extracted from the original `text` field. At first glance, you will notice that the tokenizer is not doing a great job, let us take a closer at a single record and see what was the exact result of the tokenization using the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(X[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library does a pretty decent job of tokenizing our text. There are many other tokenizers online, such as [spaCy](https://spacy.io/), and the built in libraries provided by [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). We are making use of the NLTK library because it is open source and because it does a good job of segmentating text-based data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature subset selection\n",
    "Okay, so we are making some headway here. Let us now make things a bit more interesting. We are going to do something different from what we have been doing thus far. We are going use a bit of everything that we have learned so far. Briefly speaking, we are going to move away from our main dataset (one form of feature subset selection), and we are going to generate a document-term matrix from the original dataset. In other words we are going to be creating something like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vS01RrtPHS3r1Lf8UjX4POgDol-lVF4JAbjXM3SAOU-dOe-MqUdaEMWwJEPk9TtiUvcoSqTeE--lNep/pub?w=748&h=366)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, it won't have the same shape as the table above, but we will get into that later. For now, let us use scikit learn built in functionalities to generate this document. You will see for yourself how easy it is to generate this table without much coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did with those two lines of code is that we transorfmed the articles into a **term-document matrix**. Those lines of code tokenize each article using a built-in, default tokenizer (often referred to as an `analzyer`) and then produces the word frequency vector for each document. We can create our own analyzers or even use the nltk analyzer that we previously built. To keep things tidy and minimal we are going to use the default analyzer provided by `CountVectorizer`. Let us look closely at this analyzer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = count_vect.build_analyzer()\n",
    "analyze(\"Hello World!\")\n",
    "#\" \".join(list(X[4:5].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 9 (5 min):**\n",
    "Let's analyze the first record of our X dataframe with the new analyzer we have just built. Go ahead try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the term-document matrix we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the shape of this matrix by:\n",
    "X_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can obtain the feature names of the vectorizer, i.e., the terms\n",
    "# usually on the horizontal axis\n",
    "count_vect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://i.imgur.com/57gA1sd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the features found in the all the documents `X`, which are basically all the terms found in all the documents. As I said earlier, the transformation is not in the pretty format (table) we saw above -- the term-document matrix. We can do many things with the `count_vect` vectorizer and its transformation `X_counts`. You can find more information on other cool stuff you can do with the [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). \n",
    "\n",
    "Now let us try to obtain something that is as close to the pretty table I provided above. Before jumping into the code for doing just that, it is important to mention that the reason for choosing the `fit_transofrm` for the `CountVectorizer` is that it efficiently learns the vocabulary dictionary and returns a term-document matrix.\n",
    "\n",
    "In the next bit of code, we want to extract the first five articles and transform them into document-term matrix, or in this case a 2-dimensional array. Here it goes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert from sparse array to normal array\n",
    "X_counts[0:5, 0:100].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the result is just this huge sparse matrix, which is computationally intensive to generate and difficult to visualize. But we can see that the fifth record, specifically, contains a `1` in the beginning, which from our feature names we can deduce that this article contains exactly one `00` term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 10 (take home):**\n",
    "We said that the `1` at the beginning of the fifth record represents the `00` term. Notice that there is another 1 in the same record. Can you provide code that can verify what word this 1 represents from the vocabulary. Try to do this as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the vectorizer to generate word frequency vector for new documents or articles. Let us try that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us put a `00` in the document to see if it is detected as we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(['00 Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive, huh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started in thinking about how to better analyze your data or transformation, let us look at this nice little heat map of our term-document matrix. It may come as a surpise to see the gems you can mine when you start to look at the data from a different perspective. Visualization are good for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first twenty features only\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names()[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain document index\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(X.index)[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z = X_counts[0:20, 0:20].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the heat map, we are going to use another visualization library called `seaborn`. It's built on top of matplotlib and closely integrated with pandas data structures. One of the biggest advantages of seaborn is that its default aesthetics are much more visually appealing than matplotlib. See comparison below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://i.imgur.com/1isxmIV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other big advantage of seaborn is that seaborn has some built-in plots that matplotlib does not support. Most of these can eventually be replicated by hacking away at matplotlib, but they’re not built in and require much more effort to build.\n",
    "\n",
    "So without further ado, let us try it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out more beautiful color palettes here: https://python-graph-gallery.com/197-available-color-palettes-with-matplotlib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 11 (take home):** \n",
    "From the chart above, we can see how sparse the term-document matrix is; i.e., there is only one terms with frequency of `1` in the subselection of the matrix. By the way, you may have noticed that we only selected 20 articles and 20 terms to plot the histrogram. As an excersise you can try to modify the code above to plot the entire term-document matrix or just a sample of it. How would you do this efficiently? Remember there is a lot of words in the vocab. Report below what methods you would use to get a nice and useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great thing about what we have done so far is that we now open doors to new problems. Let us be optimistic. Even though we have the problem of sparsity and a very high dimensional data, we are now closer to uncovering wonders from the data. You see, the price you pay for the hard work is worth it because now you are gaining a lot of knowledge from what was just a list of what appeared to be irrelevant articles. Just the fact that you can blow up the data and find out interesting characteristics about the dataset in just a couple lines of code, is something that truly inspires me to practise Data Science. That's the motivation right there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Dimensionality Reduction\n",
    "Since we have just touched on the concept of sparsity most naturally the problem of \"curse of dimentionality\" comes up. I am not going to get into the full details of what dimensionality reduction is and what it is good for just the fact that is an excellent technique for visualizing data efficiently (please refer to notes for more information). All I can say is that we are going to deal with the issue of sparsity with a few lines of code. And we are going to try to visualize our data more efficiently with the results.\n",
    "\n",
    "We are going to make use of Principal Component Analysis to efficeintly reduce the dimensions of our data, with the main goal of \"finding a projection that captures the largest amount of variation in the data.\" This concept is important as it is very useful for visualizing and observing the characteristics of our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA Algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "**Input:** Raw term-vector matrix\n",
    "\n",
    "**Output:** Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = PCA(n_components = 2).fit_transform(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['coral', 'blue', 'black', 'm']\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (25,10))\n",
    "ax = fig.subplots()\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = X_reduced[X['category_name'] == category].T[0]\n",
    "    ys = X_reduced[X['category_name'] == category].T[1]\n",
    "   \n",
    "    ax.scatter(xs, ys, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 2D visualization above, we can see a slight \"hint of separation in the data\"; i.e., they might have some special grouping by category, but it is not immediately clear. The PCA was applied to the raw frequencies and this is considered a very naive approach as some words are not really unique to a document. Only categorizing by word frequency is considered a \"bag of words\" approach. Later on in the course you will learn about different approaches on how to create better features from the term-vector matrix, such as term-frequency inverse document frequency so-called TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 12 (take home):\n",
    "Please try to reduce the dimension to 3, and plot the result use 3-D plot. Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "\n",
    "$Hint$: you can refer to Axes3D in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Atrribute Transformation / Aggregation\n",
    "We can do other things with the term-vector matrix besides applying dimensionalaity reduction technique to deal with sparsity problem. Here we are going to generate a simple distribution of the words found in all the entire set of articles. Intuitively, this may not make any sense, but in data science sometimes we take some things for granted, and we just have to explore the data first before making any premature conclusions. On the topic of attribute transformation, we will take the word distribution and put the distribution in a scale that makes it easy to analyze patterns in the distrubution of words. Let us get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute these frequencies for each term in all documents. Visually speaking, we are seeking to add values of the 2D matrix, vertically; i.e., sum of each column. You can also refer to this process as aggregation, which we won't explore further in this notebook because of the type of data we are dealing with. But I believe you get the idea of what that includes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vTMfs0zWsbeAl-wrpvyCcZqeEUf7ggoGkDubrxX5XtwC5iysHFukD6c-dtyybuHnYigiRWRlRk2S7gp/pub?w=750&h=412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this takes time to compute. You may want to reduce the amount of terms you want to compute frequencies for\n",
    "term_frequencies = []\n",
    "for j in range(0,X_counts.shape[1]):\n",
    "    term_frequencies.append(sum(X_counts[:,j].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300], \n",
    "            y=term_frequencies[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 13 (take home):**\n",
    "If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 14 (take home):** \n",
    "The chart above contains all the vocabulary, and it's computationally intensive to both compute and visualize. Can you efficiently reduce the number of terms you want to visualize as an exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 15 (take home):** \n",
    "Additionally, you can attempt to sort the terms on the `x-axis` by frequency instead of in alphabetical order. This way the visualization is more meaninfgul and you will be able to observe the so called [long tail](https://en.wikipedia.org/wiki/Long_tail) (get familiar with this term since it will appear a lot in data mining and other statistics courses). see picture below\n",
    "\n",
    "![alt txt](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have those term frequencies, we can also transform the values in that vector into the log distribution. All we need is to import the `math` library provided by python and apply it to the array of values of the term frequency vector. This is a typical example of attribute transformation. Let's go for it. The log distribution is a technique to visualize the term frequency into a scale that makes you easily visualize the distribution in a more readable format. In other words, the variations between the term frequencies are now easy to observe. Let us try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300],\n",
    "                y=term_frequencies_log[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides observing a complete transformation on the disrtibution, notice the scale on the y-axis. The log distribution in our unsorted example has no meaning, but try to properly sort the terms by their frequency, and you will see an interesting effect. Go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Discretization and Binarization\n",
    "In this section we are going to discuss a very important pre-preprocessing technique used to transform the data, specifically categorical values, into a format that satisfies certain criteria required by particular algorithms. Given our current original dataset, we would like to transform one of the attributes, `category_name`, into four binary attributes. In other words, we are taking the category name and replacing it with a `n` asymmetric binary attributes. The logic behind this transformation is discussed in detail in the recommended Data Mining text book (please refer to it on page 58). People from the machine learning community also refer to this transformation as one-hot encoding, but as you may become aware later in the course, these concepts are all the same, we just have different prefrence on how we refer to the concepts. Let us take a look at what we want to achieve in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.fit(X.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['bin_category'] = mlb.transform(X['category']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the new attribute we have added to the `X` table. You can see that the new attribute, which is called `bin_category`, contains an array of 0's and 1's. The `1` is basically to indicate the position of the label or category we binarized. If you look at the first two records, the one is places in slot 2 in the array; this helps to indicate to any of the algorithms which we are feeding this data to, that the record belong to that specific category. \n",
    "\n",
    "Attributes with **continuous values** also have strategies to tranform the data; this is usually called **Discretization** (please refer to the text book for more inforamation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 16 (take home):**\n",
    "Try to generate the binarization using the `category_name` column instead. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to take a peek at your data to understand the relationships in your dataset. Here, we will focus in a similarity example. Let's take 3 documents and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve 2 sentences for a random record, here, indexed at 50 and 100\n",
    "document_to_transform_1 = []\n",
    "random_record_1 = X.iloc[50]\n",
    "random_record_1 = random_record_1['text']\n",
    "document_to_transform_1.append(random_record_1)\n",
    "\n",
    "document_to_transform_2 = []\n",
    "random_record_2 = X.iloc[100]\n",
    "random_record_2 = random_record_2['text']\n",
    "document_to_transform_2.append(random_record_2)\n",
    "\n",
    "document_to_transform_3 = []\n",
    "random_record_3 = X.iloc[150]\n",
    "random_record_3 = random_record_3['text']\n",
    "document_to_transform_3.append(random_record_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_to_transform_1)\n",
    "print(document_to_transform_2)\n",
    "print(document_to_transform_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Transform sentence with Vectorizers\n",
    "document_vector_count_1 = count_vect.transform(document_to_transform_1)\n",
    "document_vector_count_2 = count_vect.transform(document_to_transform_2)\n",
    "document_vector_count_3 = count_vect.transform(document_to_transform_3)\n",
    "\n",
    "# Binarize vecors to simplify: 0 for abscence, 1 for prescence\n",
    "document_vector_count_1_bin = binarize(document_vector_count_1)\n",
    "document_vector_count_2_bin = binarize(document_vector_count_2)\n",
    "document_vector_count_3_bin = binarize(document_vector_count_3)\n",
    "\n",
    "# print\n",
    "print(\"Let's take a look at the count vectors:\")\n",
    "print(document_vector_count_1.todense())\n",
    "print(document_vector_count_2.todense())\n",
    "print(document_vector_count_3.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cos_sim_count_1_2 = cosine_similarity(document_vector_count_1, document_vector_count_2, dense_output=True)\n",
    "cos_sim_count_1_3 = cosine_similarity(document_vector_count_1, document_vector_count_3, dense_output=True)\n",
    "cos_sim_count_1_1 = cosine_similarity(document_vector_count_1, document_vector_count_1, dense_output=True)\n",
    "cos_sim_count_2_2 = cosine_similarity(document_vector_count_2, document_vector_count_2, dense_output=True)\n",
    "\n",
    "# Print \n",
    "print(\"Cosine Similarity using count bw 1 and 2: %(x)f\" %{\"x\":cos_sim_count_1_2})\n",
    "print(\"Cosine Similarity using count bw 1 and 3: %(x)f\" %{\"x\":cos_sim_count_1_3})\n",
    "print(\"Cosine Similarity using count bw 1 and 1: %(x)f\" %{\"x\":cos_sim_count_1_1})\n",
    "print(\"Cosine Similarity using count bw 2 and 2: %(x)f\" %{\"x\":cos_sim_count_2_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, cosine similarity between a sentence and itself is 1. Between 2 entirely different sentences, it will be 0. \n",
    "\n",
    "We can assume that we have the more common features in bthe documents 1 and 3 than in documents 1 and 2. This reflects indeed in a higher similarity than that of sentences 1 and 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We have come a long way! We can now call ourselves experts of Data Preprocessing. You should feel excited and proud because the process of Data Mining usually involves 70% preprocessing and 30% training learning models. You will learn this as you progress in the Data Mining course. I really feel that if you go through the exercises and challenge yourself, you are on your way to becoming a super Data Scientist. \n",
    "\n",
    "From here the possibilities for you are endless. You now know how to use almost every common technique for preprocessing with state-of-the-art tools, such as as Pandas and Scikit-learn. You are now with the trend! \n",
    "\n",
    "After completing this notebook you can do a lot with the results we have generated. You can train algorithms and models that are able to classify articles into certain categories and much more. You can also try to experiment with different datasets, or venture further into text analytics by using new deep learning techniques such as word2vec. All of this will be presented in the next lab session. Until then, go teach machines how to be intelligent to make the world a better place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas cook book ([Recommended for starters](http://pandas.pydata.org/pandas-docs/stable/cookbook.html))\n",
    "- [Pang-Ning Tan, Michael Steinbach, Vipin Kumar, Introduction to Data Mining, Addison Wesley](https://dl.acm.org/citation.cfm?id=1095618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}